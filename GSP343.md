# Optimize Costs for Google Kubernetes Engine: Challenge Lab

>Here are some guidelines you've been requested to follow when deploying:
>
> - Create the cluster in the `us-central1` region
> - The naming scheme is team-resource, e.g. a cluster could be named `onlineboutique-cluster`
> - For your initial cluster, start with machine size `n1-standard-2 (2 vCPU, 8G memory)`

Set your default zone/region:
>`gcloud config set compute/region us-central1`

>`gcloud config set compute/zone us-central1-a`

Configure tab completion for the kubectl command-line tool.
>`source <(kubectl completion bash)`

## Task 1: Create our cluster and deploy our app
Create a zonal cluster with two (2) nodes:
>`gcloud container clusters create onlineboutique-cluster \`\
>`--zone us-central1-a  --machine-type n1-standard-2 --num-nodes 2`

Wait for the GKE cluster to be created. Once created, set up the `dev` and `prod` namespaces to separate resources on the cluster.
>`kubectl create namespace dev`

>`kubectl create namespace prod`

### Deploy application to the `dev` namespace
#### 1. Copy the application source code from the GitHub repository to Cloud Shell:
>`git clone https://github.com/GoogleCloudPlatform/microservices-demo.git`

#### 2. Go to the **microservices-demo** directory where we will find the yaml file.
>`cd microservices-demo`

#### 3. Deploy the app, not forgetting to indicate the namespace.
>`kubectl apply -f ./release/kubernetes-manifests.yaml --namespace dev`

#### 4. Wait awhile for deployment to complete, check the **frontend-external** LoadBalancer service and whether its External IP is ready by running:
>`kubectl get services --namespace dev`

Once the External IP is avaialble and shown, copy the address and paste it in a new tab. You should see the hompeage of Online Boutique.

>**Note**: To omit typing `--namespace dev` parameter for every command, you may want to switch to the `dev` namespace.
>
>`kubectl config set-context --current --namespace=dev`
>
>This will create all Kubernetes objects in `dev` namespace by default unless explicity stated otherwise. You may choose to switch back to the default or other namespace using similar command `kubectl config set-context --current --namespace [NAMESPACE]`.


## Task 2: Migrate to an Optimized Nodepool
Create a new node pool named **optimized-pool** with **custom-2-3584** as the machine type. Set the **number of nodes** to **2**.
>`gcloud container node-pools create optimized-pool \`\
>`--zone us-central1-a --machine-type custom-2-3584 --num-nodes 2 \`\
>`--cluster onlineboutique-cluster`

Once the new nodepool is set up, migrate your application's deployments to the new nodepool by **cordoning off and draining** `default-pool`. 
```
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl cordon "$node";
done

for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
   kubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 "$node";
done
```

At this point, you should see that your pods are running on the new, `optimized-pool`, node pool:
>`kubectl get pods -o wide`

With the pods migrated, it's safe to **delete** the old node pool:
>`gcloud container node-pools delete default-pool --cluster onlineboutique-cluster --zone us-central1-a`

Type **y** and **ENTER** to continue.

## Task 3: Apply a Frontend Update
Set a pod disruption budget for your **frontend** deployment. Name it **onlineboutique-frontend-pdb** and set the **min-availability** of your deployment to **1**.
>`kubectl create poddisruptionbudget onlineboutique-frontend-pdb --namespace dev --selector app=frontend --min-available 1`

Check the pod disruption budget has been created:
>`kubectl get poddisruptionbudgets`

Now, apply the team's update. The file used for the home page's banner has changed and there is an updated docker image:
```
gcr.io/qwiklabs-resources/onlineboutique-frontend:v2.1
```

**Edit** the **frontend** deployment and change its image to the updated one:
>`KUBE_EDITOR="nano" kubectl edit deployment/frontend --namespace=dev`

Change the parameter value for **image** to `gcr.io/qwiklabs-resources/onlineboutique-frontend:v2.1` and **imagePullPolicy** to `Always`.

Press **CTRL+X** then **y** and **Enter** to save the changes and exit.

You may verify the update applied by going to Cloud Console **Navigation Menu > Kubernetes Engine > Workloads** and click on `frontend` deployment. Under **REVISION HISTORY**, the latest revision should show `onlineboutique-frontend:v2.1`.

## Task 4: Autoscale from Estimated Traffic
### Apply horizontal pod autoscaling to the frontend deployment
To scale base off a **target cpu percentage of 50** and set the **pod scaling between 1 minimum** and **13 maximum**:
>`kubectl autoscale deployment frontend --namespace dev --cpu-percent 50 --min 1 --max 13`

Verify the autoscaling property set:
>`kubectl get hpa --namespace dev`

### Apply cluster autoscaling to onlineboutique-cluster
Update your **cluster autoscaler** to scale between **1 node minimum** and **6 nodes maximum**.
>`gcloud beta container clusters update onlineboutique-cluster --zone us-central1-a \`\
>`--enable-autoscaling --min-nodes 1 --max-nodes 6`

### Run a load test to simulate the traffic surge 
**OnlineBoutique** was designed with built-in load generation. Currently, your dev instance is simulating traffic on the store with ~10 concurrent users.

Run the load generation from the `loadgenerator` pod with a higher number of concurrent user:
```
kubectl exec $(kubectl get pod --namespace=dev | grep 'loadgenerator' | cut -f1 -d ' ') -it --namespace=dev -- bash -c "export USERS=8000; sh ./loadgen.sh"
```

Observe the **Workloads** and monitor how the claster handles traffic spike. You should see `recommendationservice` crashing or, at least, heavily struggling from the increased demand.

### Apply horizontal pod autoscaling to your recommendationservice deployment. 
To scale base off a **target cpu percentage of 50** and set the **pod scaling between 1 minimum** and **5 maximum**:
>`kubectl autoscale deployment/recommendationservice --namespace dev --cpu-percent 50 --min 1 --max 5`

## Congratulations!
You have completed the lab. Check progress to verify all tasks achieved and you may now end the lab. ðŸ˜€
